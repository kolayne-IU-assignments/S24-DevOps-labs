# StatefulSet

## Status

```sh
$ kubectl get po,sts,svc,pvc
NAME           READY   STATUS    RESTARTS   AGE
pod/app-py-0   1/1     Running   0          54s
pod/app-py-1   1/1     Running   0          51s
pod/app-py-2   1/1     Running   0          42s
pod/app-py-3   1/1     Running   0          40s

NAME                      READY   AGE
statefulset.apps/app-py   4/4     54s

NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/app-py       LoadBalancer   10.110.165.118   <pending>     5000:32326/TCP   54s
service/kubernetes   ClusterIP      10.96.0.1        <none>        443/TCP          50m

NAME                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/persistent-app-py-0   Bound    pvc-9379a89d-3019-4388-a875-15b697628f73   1Mi        RWOP           standard       42m
persistentvolumeclaim/persistent-app-py-1   Bound    pvc-940db74b-8df6-46a9-a62a-f73ea13b3082   1Mi        RWOP           standard       42m
persistentvolumeclaim/persistent-app-py-2   Bound    pvc-9294452e-e02a-464f-9bc6-e5d734755501   1Mi        RWOP           standard       42m
persistentvolumeclaim/persistent-app-py-3   Bound    pvc-5e5d2589-d20e-4ea3-a21e-d199b01b4629   1Mi        RWOP           standard       42m
$
```

```sh
$ kubectl exec pod/app-py-0 -- cat /app/persistent/visits.bin | python3 -c 'from sys import stdin; print(int.from_bytes(stdin.buffer.read(), "little"))'
Defaulted container "app-py" out of: app-py, change-ownership-of-persistent-volume (init)
114
$ kubectl exec pod/app-py-1 -- cat /app/persistent/visits.bin | python3 -c 'from sys import stdin; print(int.from_bytes(stdin.buffer.read(), "little"))'
Defaulted container "app-py" out of: app-py, change-ownership-of-persistent-volume (init)
95
$ kubectl exec pod/app-py-2 -- cat /app/persistent/visits.bin | python3 -c 'from sys import stdin; print(int.from_bytes(stdin.buffer.read(), "little"))'
Defaulted container "app-py" out of: app-py, change-ownership-of-persistent-volume (init)
95
$ kubectl exec pod/app-py-3 -- cat /app/persistent/visits.bin | python3 -c 'from sys import stdin; print(int.from_bytes(stdin.buffer.read(), "little"))'
Defaulted container "app-py" out of: app-py, change-ownership-of-persistent-volume (init)
95
$
```

It appears most likely that every time I access the web page from my browsers, the requests are
forwarded to the pod/app-py-0, in addition to that, health checks from kubernetes access each
of the pods, thus resulting in an additional increment equal for all other pods.

## Ordering guarantee and parallel operations

While being stateful (that is, requiring a piece of persistent storage), instances of app
are independent of each other: every pod counts visits to itself, and these values are not
affected by any activities in other pods.

As there's no relations between pods, it doesn't matter in what order to start them, it's even
possible to start them in parallel without a deterministic order.

## Update strategies

Update strategies are ways to go from an existing kubernetes deployment to the next one,
when the configuration of kubernetes components change. Here are some strategies

### Rolling update

In a rolling update, kubernetes replaces every pod in the deployment one by one, so that
the cluster remains available and the update is imperceptible for an outside user.

Via the `MaxSurge` and `MaxUnavailable` parameters one can control the pace at which pod
replacements are performed (so that there's more than one pod replaced at a time).

#### Rampled slow rollout

A special case of rolling update with a positive `MaxSurge` (e.g., for `MaxSurge=1`,
pods are rolled out one at a time) and `MaxUnavailable=0` (that is, there are always enough
available pods to meet the replicas requirement).

#### Best-effort controlled rollout

A special case of rolling update with a positive `MaxUnavailable` (that is, a certain downtime
percentage one can tolerate) and `MaxSurge=0` (ensures there is a fixed number of pods in one's
Deployment, maximizing resource utilization in the update process).

### Recreate deployment

Recreate deployment is the most straightforward strategy, which is to stop all previously
deployed pods, then spawn new ones, with some downtime in between.

### Blue-Green deployment

Given the first deployment (marked blue), deploy the new deployment (marked green) alongside,
while continuing to route traffic to the blue one. Once green is fully deployed (and, optionally,
tested), reroute traffic to green.

### Canary deployment

The new deployment is deployed alongside the old deployment; users are gradually shifted from
the old app version to the new one. This allows to test updates on real users and detect problems
(if any) without affecting most users.

### A/B testing

The new deployment is deployed alongside the old deployment. Traffic for some users is routed
to the old deployment, meanwhile other users access the new version. Based on metrics defined
in advance, developers can compare the behavior of the old and new versions.

### Shadow deployment

The new deployment is deployed alongside the old deployment. Users traffic is routed to both
the old and the new versions but users only receive response from the old deployment. Much
like the two previous options, this strategy allows to test the new version of the app on real
data but does not affect end-users.
